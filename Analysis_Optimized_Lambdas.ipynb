{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from deliverable.code.tsp.TSP_Formulation_Methods import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "distances_original_matrix = np.loadtxt(\"./data/matriz-rutas-granada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, we load a set of computed lambdas (all if not specified) and do some statistical analysis on them\n",
    "# The lambdas computed are all from staring factor 0.01 and 5 iterations (see QUBO_optimized_lambdas.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_simulations = \"lambdas\"\n",
    "files_to_find = \"./data/lamdasOptimized/\"+concrete_simulations+\"*\"\n",
    "files = glob.glob(files_to_find)\n",
    "\n",
    "all_weights = []\n",
    "\n",
    "for file in files:\n",
    "    weights = np.loadtxt(file)\n",
    "    all_weights.append(weights)\n",
    "\n",
    "all_weights = np.array(all_weights)  # Shape: (num_files, 5)\n",
    "\n",
    "means = np.mean(all_weights, axis=0)\n",
    "variances = np.var(all_weights, axis=0)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Weight λ{i+1}: Mean = {means[i]:.4f}, Variance = {variances[i]:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    plt.hist(all_weights[:, i], bins=10, alpha=0.6, label=f\"λ{i+1}\")\n",
    "plt.xlabel(\"Valor del peso\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Distribución de los pesos óptimos\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(all_weights, labels=[f\"λ{i+1}\" for i in range(5)])\n",
    "plt.ylabel(\"Weight value\")\n",
    "plt.title(\"Variability of optimal weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we try to solve using the mean lambdas. We check the accuracy for each kind of combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "p = 2\n",
    "num_reads_solver = 400\n",
    "distances_N_stops_normalized = distances_original_matrix[:N,:N]/np.max(distances_original_matrix[:N,:N])\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "concrete_simulations = \"lambdas\"\n",
    "files_to_find = os.path.join(script_dir, f\"data/lamdasOptimized/{concrete_simulations}*\")\n",
    "list_of_files = glob.glob(files_to_find)\n",
    "\n",
    "mean_lambdas = load_lambda_means(list_of_files)\n",
    "\n",
    "print(\"Mean lambdas: \", mean_lambdas)\n",
    "\n",
    "all_start_end_combinations = generate_all_start_end_combinations(N, L=1)\n",
    "\n",
    "solutions_analysis = np.zeros((len(all_start_end_combinations), 3))\n",
    "\n",
    "for index, (startNodes, endNodes) in enumerate(all_start_end_combinations):\n",
    "    startNode = startNodes[0]\n",
    "    endNode = endNodes[0]\n",
    "    solutions_analysis[index][0] = startNode\n",
    "    solutions_analysis[index][1] = endNode\n",
    "    Q_matrix,_ = create_QUBO_matrix(distances_N_stops_normalized, p, startNode, endNode, mean_lambdas)\n",
    "    solution_array, _ = solve_qubo_with_Dwave(Q_matrix, num_reads_solver)\n",
    "    solutions_analysis[index][2] = int(check_solution_return(solution_array, N, p, startNode, endNode))\n",
    "\n",
    "show_statistics_lambdas(solutions_analysis, N)\n",
    "\n",
    "check_solution(solution_array, N, p, N-2, N-1)\n",
    "draw_solution_graph(solution_array, distances_N_stops_normalized, p, solutions_analysis[-1][0], solutions_analysis[-1][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we visualize the value for each optimized penalty weight individualized for N and p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "N_min = 2\n",
    "N_max = 4\n",
    "lambdas_dict = {}\n",
    "for n in range(N_min, N_max + 1):\n",
    "    for p in range(1, n):\n",
    "        script_dir = os.getcwd()\n",
    "        concrete_simulations = f\"lambdas_N_{n}_p_{p}\"\n",
    "        files_to_find = os.path.join(script_dir, f\"data/lamdasOptimized/{concrete_simulations}*\")\n",
    "        list_of_files = glob.glob(files_to_find)\n",
    "        lambdas_dict[(n, p)] = np.array(load_lambda_means(list_of_files))\n",
    "\n",
    "# Define the exceptions\n",
    "exceptions = [(5,4), (6,4), (6,5)]\n",
    "# Create the figure and axes\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Colors for different values of p\n",
    "colors = {1: 'yellow', 2: 'red', 3: 'blue', 4: 'green', 5: 'purple'}\n",
    "labels_handled = set()\n",
    "\n",
    "# For each weight we store the maximum value found for all N and p\n",
    "\n",
    "# Calculate the maximum value for each weight\n",
    "max_values = np.zeros(5)\n",
    "for key in lambdas_dict.keys():\n",
    "    if key not in exceptions:\n",
    "        max_values = np.maximum(max_values, np.abs(lambdas_dict[key]))\n",
    "\n",
    "# Plot each weight on a different axis\n",
    "for weight_index in range(5):\n",
    "    # Dictionary to store points for each value of p\n",
    "    points_by_p = {p: ([], []) for p in colors.keys()}\n",
    "\n",
    "    for key in lambdas_dict.keys():\n",
    "        if key not in exceptions:\n",
    "            n, p = key\n",
    "            weight_value = lambdas_dict[key][weight_index]\n",
    "            points_by_p[p][0].append(n)\n",
    "            points_by_p[p][1].append(weight_value)\n",
    "            label = f'p={p}' if p not in labels_handled else \"\"\n",
    "            labels_handled.add(p)\n",
    "\n",
    "    # Plot the points and lines on the corresponding axis\n",
    "    ax = axes[weight_index]\n",
    "    for p, (x, y) in points_by_p.items():\n",
    "        ax.plot(x, y, 'o-', color=colors[p], label=f'p={p}')\n",
    "    \n",
    "    # Add legend and labels\n",
    "    ax.set_title(f\"Weight λ{weight_index + 1} for different N and p\")\n",
    "    ax.set_xlabel(\"N\")\n",
    "    ax.set_ylabel(f\"Value of Weight λ{weight_index + 1}\")\n",
    "    ax.grid(True)\n",
    "\n",
    "# Add the legend only once\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right')\n",
    "\n",
    "print(\"Upper bound distances by factor 0.01: \", calculate_upper_bound_distances(0.01*distances_N_stops_normalized, p))\n",
    "print(\"Maximum values of the weights: \", max_values) \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As a certain tendency is observed (generalized decreasing in the values or stabilization in certain values), we can set the penalty weights to be the upper bound (max_values) observed and check how accurate is this choice for non computed optimized lamndas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, an overview fixing N and p\n",
    "\n",
    "N = 5 # Number of stops\n",
    "p = 2 # Number of travels, aka number of edges. The number of involucred stops is then p+1\n",
    "startNode = np.random.randint(0, N-1)\n",
    "endNode = np.random.randint(startNode+1, N)\n",
    "num_reads_solver = 400\n",
    "\n",
    "R = N*(p+1)\n",
    "distances_N_stops_normalized = distances_original_matrix[:N,:N]/np.max(distances_original_matrix[:N,:N])\n",
    "\n",
    "lambdas = [max_values[i]-0.5 for i in range(5)] # We try to minmize the weights\n",
    "Q_matrix,_ = create_QUBO_matrix(distances_N_stops_normalized, p, startNode, endNode, lambdas)\n",
    "\n",
    "if R <= 20:\n",
    "    combinations_zipped = brute_force_finding(Q_matrix, distances_N_stops_normalized, p)\n",
    "    minimal_solution = np.array(list(combinations_zipped[0][0]), dtype=int)\n",
    "\n",
    "    show_parameters_of_solution(minimal_solution, distances_N_stops_normalized, N, p, startNode, endNode)\n",
    "\n",
    "    draw_solution_graph(minimal_solution, distances_N_stops_normalized, p, startNode, endNode)\n",
    "\n",
    "    plot_brute_force_minimums(combinations_zipped, N, p, startNode, endNode, rangePlot=20)\n",
    "\n",
    "else:\n",
    "    solution_array, _ = solve_qubo_with_Dwave(Q_matrix, num_reads_solver)\n",
    "\n",
    "    check_solution(solution_array, N, p, startNode, endNode)\n",
    "\n",
    "    show_parameters_of_solution(solution_array, distances_N_stops_normalized, N, p, startNode, endNode)\n",
    "\n",
    "    draw_solution_graph(solution_array, distances_N_stops_normalized, p, startNode, endNode)\n",
    "\n",
    "    matrix_cost_solution = calculate_cost(solution_array, Q_matrix)\n",
    "\n",
    "    print(\"\\nTotal Matrix Cost of solution:\", matrix_cost_solution)\n",
    "\n",
    "    random_solution = np.random.randint(0, 2, size=R)\n",
    "\n",
    "    print(\"\\nTotal Matrix Cost of random solution:\", calculate_cost(random_solution, Q_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we move systematically through different values of N and p and store for each if a valid solution is found\n",
    "\n",
    "lambdas = [np.abs(max_values[i]-0.5) for i in range(5)] # We decrease a lot the max weights to see for which N the solutions start ti fail\n",
    "\n",
    "min_N = 2\n",
    "max_N = 10\n",
    "num_reads_solver = 100\n",
    "\n",
    "count_total_solutions = (max_N * (max_N + 1)) // 2 - ((min_N - 1) * min_N) // 2 - (max_N - min_N + 1)\n",
    "solutions_analysis = [] \n",
    "\n",
    "for n in range(min_N, max_N+1):\n",
    "    for p in range(1, n):\n",
    "        startNode = np.random.randint(0, n-1) # Note that each time the code is run, the start and end nodes are randomly selected\n",
    "        endNode = np.random.randint(startNode+1, n)\n",
    "        \n",
    "        distances_N_stops_normalized = distances_original_matrix[:n,:n] / np.max(distances_original_matrix[:n,:n])\n",
    "\n",
    "        Q_matrix, _ = create_QUBO_matrix(distances_N_stops_normalized, p, startNode, endNode, lambdas)\n",
    "\n",
    "        solution_array, _ = solve_qubo_with_Dwave(Q_matrix, num_reads_solver)\n",
    "\n",
    "        is_valid = int(check_solution_return(solution_array, n, p, startNode, endNode))\n",
    "\n",
    "        # Agregar los datos a la lista\n",
    "        solutions_analysis.append([n, p, startNode, endNode, solution_array, is_valid])\n",
    "\n",
    "count_valid_solutions = sum(1 for sol in solutions_analysis if sol[5] == 1)\n",
    "\n",
    "print(f\"Total number of solutions found: {count_total_solutions}\")\n",
    "print(f\"Total number of valid solutions found: {count_valid_solutions}\")\n",
    "\n",
    "valid_solutions_per_N = []\n",
    "no_valid_solutions_per_N = []\n",
    "for n in range(min_N, max_N+1):\n",
    "    sum_valid_solutions = sum(1 for sol in solutions_analysis if sol[0] == n and sol[5] == 1)\n",
    "    sum_all_solutions = sum(1 for sol in solutions_analysis if sol[0] == n)\n",
    "    valid_solutions_per_N.append(sum_valid_solutions)\n",
    "    no_valid_solutions_per_N.append(sum_all_solutions - sum_valid_solutions)\n",
    "\n",
    "valid_solutions_per_p = []\n",
    "no_valid_solutions_per_p = []\n",
    "for p in range(1, max_N):\n",
    "    sum_valid_solutions = sum(1 for sol in solutions_analysis if sol[1] == p and sol[5] == 1)\n",
    "    sum_all_solutions = sum(1 for sol in solutions_analysis if sol[1] == p)\n",
    "    valid_solutions_per_p.append(sum_valid_solutions)\n",
    "    no_valid_solutions_per_p.append(sum_all_solutions - sum_valid_solutions)\n",
    "\n",
    "# Plot the results in two different axes\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].bar(range(min_N, max_N+1), valid_solutions_per_N, color='green', label='Valid solutions')\n",
    "axes[0].bar(range(min_N, max_N+1), no_valid_solutions_per_N, bottom=valid_solutions_per_N, color='red', label='No valid solutions')\n",
    "axes[0].set_xlabel('Number of stops N')\n",
    "axes[0].set_ylabel('Number of solutions')\n",
    "axes[0].set_title('Number of valid and no valid solutions found by N')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].bar(range(1, max_N), valid_solutions_per_p, color='green', label='Valid solutions')\n",
    "axes[1].bar(range(1, max_N), no_valid_solutions_per_p, bottom=valid_solutions_per_p, color='red', label='No valid solutions')\n",
    "axes[1].set_xlabel('Number of travels p')\n",
    "axes[1].set_ylabel('Number of solutions')\n",
    "axes[1].set_title('Number of valid and no valid solutions found by p')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIONS:\n",
    "\n",
    "We can see that the solutions are more sensitive to the weights when p is close to N, that is why for lower N the solutions start to fail earlier.\n",
    "This is a promising behaviour since it means that one can compute the optimal lambdas for the first few N, which takes low computational times in the brute force finding and then use these lambdas for higher N, where the solutions are less sensitive to the weights. This is a good strategy to reduce computational times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expertcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
